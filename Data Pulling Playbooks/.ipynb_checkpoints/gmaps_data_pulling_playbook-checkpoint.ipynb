{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198b6d75-c265-42f3-94e3-0e6b106d37b5",
   "metadata": {},
   "source": [
    "# Google Places Data Setup Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e925c-f6c3-4d99-bde7-15c4bbc9bebc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naming Conventions in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a90e4-2a8d-43d6-b62f-47149c16d4a6",
   "metadata": {},
   "source": [
    "All Google amenities data used in this framework are stored in the homevest-data.nbhd_similarity dataset with the following naming convention. Unlike yelp places data, the yelp API pulls by CBSA code/at the CBSA scope, and thus is named as such. \n",
    "\n",
    "* “gmaps_{KEYWORD}_{STATE FIPS}{COUNTYFIPS}”\n",
    "* ex:\n",
    "    * “gmaps_arcade_13063”\n",
    "    * “gmaps_bail_bonds_13063”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a52a7-d6e7-47de-9368-9cb8bd1abadf",
   "metadata": {},
   "source": [
    "## Use the following code to pull new data from Google Places into BigQuery. \n",
    "\n",
    "Make sure to follow through and store the pulled data to bigquery so we don't have to pull the data & be charged for it again. Data is first pulled from the api insto GCS, and other functions are used to then stored that GCS data into BigQuery.\n",
    "\n",
    "*The final functions section has the functions you can run to do all the work of searching, calling, and storing for you, but you will need to run the lines above it for set up!.*\n",
    "\n",
    "**Be aware of Google Places API costs!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e172d-a623-4860-8c65-e75fb3fc382e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6439005b-7522-4897-918c-216070093e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "import censusdata\n",
    "from us import states\n",
    "from google.cloud import bigquery, storage\n",
    "import time\n",
    "import logging\n",
    "from google.cloud.exceptions import NotFound\n",
    "from shapely.geometry import Point # Shapely for converting latitude/longtitude to geometry\n",
    "import geopandas as gpd # To create GeodataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114978d-0d1c-4e11-8ec7-86b20b80e108",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Geodata pulling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ff1a5a-4424-48c8-b728-a9452a9e0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull bigquery query as dataframe function\n",
    "def bq_to_df(query):\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=\"US\")\n",
    "    df = query_job.to_dataframe()\n",
    "    return df\n",
    "\n",
    "def state_fips(state): #pulls state fips\n",
    "    return states.lookup(state).fips\n",
    "\n",
    "def pull_county_zip_codes(state, county): #pulls all zip codes in county\n",
    "    zipcode_points_query = f\"\"\"\n",
    "    WITH county AS (SELECT * \n",
    "    FROM `bigquery-public-data.geo_us_boundaries.counties` cbsa\n",
    "    WHERE geo_id = '{str(state_fips(state)) + str(county)}')\n",
    "\n",
    "    SELECT zc.zip_code,\n",
    "    zc.internal_point_lat as latitude,\n",
    "    zc.internal_point_lon as longitude,\n",
    "    FROM `bigquery-public-data.geo_us_boundaries.zip_codes` zc\n",
    "    CROSS JOIN county c \n",
    "    WHERE zc.state_fips_code = '{state_fips(state)}' AND\n",
    "    ST_INTERSECTS(zc.zip_code_geom, c.county_geom) AND \n",
    "    NOT ST_TOUCHES(zc.zip_code_geom, c.county_geom)\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(location=\"US\")\n",
    "    query_job = client.query(\n",
    "        zipcode_points_query,\n",
    "        location=\"US\")\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pull_census_tract_geodata(state,county=False): #pulls all census tracts in county or entire state\n",
    "    q = f\"\"\"SELECT * \n",
    "      EXCEPT (internal_point_geo, tract_geom)\n",
    "    FROM `bigquery-public-data.geo_census_tracts.us_census_tracts_national` \n",
    "    WHERE county_fips_code = '{county}' AND\n",
    "    state_fips_code = '{state_fips(state)}'\n",
    "    \"\"\"\n",
    "    df = bq_to_df(q).rename(columns={'tract_ce':'tract','county_fips':'county','internal_point_lat':'latitude','internal_point_lon':'longitude'})\n",
    "    df.latitude = df.latitude.str.replace('+','')\n",
    "    df.longitude = df.longitude.str.replace('+','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fc237-46ab-4c8f-be71-b6e4883f96cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Setting Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa33a322-c462-4bcb-b693-6824e08cac67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ENTER_HERE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2907/2315146226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mENTER_HERE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mBASE_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mGCS_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'homevest-data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mGCS_DATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nbhd_similarity/45045/'\u001b[0m \u001b[0;31m#change county here everytime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENTER_HERE' is not defined"
     ]
    }
   ],
   "source": [
    "API_KEY = {ENTER_HERE}\n",
    "BASE_URL = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "GCS_BUCKET = 'homevest-data'\n",
    "GCS_DATA_DIR = 'nbhd_similarity/45045/' #change county here everytime\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(GCS_BUCKET)\n",
    "client = bigquery.Client(location=\"US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9858d59a-679e-4789-ab4f-b57bad429aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pulling and Storing Results in GCS Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c31376-0f68-4ce4-a003-493976bb6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if gc file exits already \n",
    "def is_gcs_file_existing(file_name, directory, bucket):\n",
    "    blob = bucket.blob(f'{directory}{file_name}')\n",
    "    return blob.exists()\n",
    "    \n",
    "# upload file to gcs\n",
    "def upload_dict_to_gcs(dict_to_upload, file_name, directory, bucket):    \n",
    "    json_string = json.dumps(dict_to_upload)\n",
    "    blob = bucket.blob(f'{directory}{file_name}')\n",
    "    blob.upload_from_string(json_string)\n",
    "\n",
    "# gets next page of results\n",
    "def get_next_page(json_file):\n",
    "    token = json_file['next_page_token']\n",
    "    r_next_page = requests.get(BASE_URL, params = {'pagetoken': token, 'key' : API_KEY})\n",
    "    r_next_page.raise_for_status()\n",
    "    return r_next_page.json()\n",
    "\n",
    "# pulls results given a df, a keyword, and a scope\n",
    "def pull_keyword_results(input_df, keyword, scope='zip_code', replace=False, verbose=True, max_locs=1500,):\n",
    "    # initialization\n",
    "    #set parameters\n",
    "    params = {\n",
    "        'location' : None,\n",
    "        'keyword' : keyword,\n",
    "        'key' : API_KEY,\n",
    "        'rankby' : 'distance'\n",
    "    }\n",
    "    errors = []\n",
    "    total_locs = len(input_df) #find total number of locations (either zip codes or census tracts)\n",
    "    \n",
    "    if total_locs > max_locs: #check to see if total number of provided tracts or zips is over max \n",
    "        raise ValueError(f'# of zip codes in MSA({total_locs}) exceeds maximum of {max_locs}!')\n",
    "    \n",
    "    for idx, row in input_df.iterrows():\n",
    "        loc = row[scope] #pulls either the tract name or the zipcode depending on specified scope\n",
    "        \n",
    "        file_path = f'{keyword.lower().replace(\" \", \"_\")}/{loc}.json' #file named based on loc\n",
    "        \n",
    "        if not replace and is_gcs_file_existing(file_path, GCS_DATA_DIR, bucket): #so long as not replacing and file not already exist, build the tables\n",
    "            continue\n",
    "            \n",
    "        params['location'] =  f\"{row['latitude']}, {row['longitude']}\" #pull exact geocode for loc\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(BASE_URL, params=params)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            upload_dict_to_gcs(r.json(), file_path, GCS_DATA_DIR, bucket) #upload data to gcs\n",
    "            \n",
    "            # Thu include next page token\n",
    "            i = 0\n",
    "            api_result = []\n",
    "            api_result.append(r.json())\n",
    "            next_page = 'next_page_token' in api_result[0] # If there's a next page token, then this will be true and the while loop will run\n",
    "            while next_page: # The below runs if true and doesn't run if false\n",
    "                time.sleep(2.4)\n",
    "                i = i + 1\n",
    "                file_path = f'{keyword.lower().replace(\" \", \"_\")}/{loc}_{i}.json' # Append number to indicate file order\n",
    "                # print(file_path)\n",
    "                x = get_next_page(api_result[0])\n",
    "                # print (x)\n",
    "                upload_dict_to_gcs(x, file_path, GCS_DATA_DIR, bucket)\n",
    "                api_result.clear()\n",
    "                api_result.append(x)\n",
    "                next_page = 'next_page_token' in api_result[0] # Update next page status, if it false then while loop stops running\n",
    "        \n",
    "        except BaseException:\n",
    "            logging.exception(\"An exception was thrown!\")\n",
    "            print(f'Error encountered for zip code {loc}')\n",
    "            errors.append(loc)\n",
    "        \n",
    "        if verbose and idx % 30 == 0:\n",
    "            print(f'{idx} of {total_locs} {scope} processed')\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211d5d9-216c-4731-b038-986fffd9ed68",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Conjoin GCS Files and Upload Final File to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d319c58b-04fa-42d2-8e17-dba6e1530559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we want to pull\n",
    "COLUMNS=[\n",
    "   'place_id',\n",
    "    'name',\n",
    "    'rating',\n",
    "    'user_ratings_total',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'icon',\n",
    "    'types',\n",
    "    'business_status'\n",
    "]\n",
    "\n",
    "# pulls file paths from gcs directory \n",
    "def get_file_paths_from_gcs_directory(keyword, bucket=bucket): \n",
    "    blobs = bucket.list_blobs(prefix = f'{GCS_DATA_DIR}{keyword.lower().replace(\" \", \"_\")}/')\n",
    "    file_path_list = []\n",
    "    for blob in blobs:\n",
    "        file_path_list.append(blob.name)\n",
    "        df = pd.DataFrame({'file_path' : file_path_list})\n",
    "    return df\n",
    "\n",
    "#pulls jsons of given file path from gcs\n",
    "def get_json_per_file_path(file_path, keyword, bucket=bucket):\n",
    "    blob = bucket.blob(f'{file_path}')\n",
    "    json_string = blob.download_as_string()\n",
    "\n",
    "    return json.loads(json_string)\n",
    "\n",
    "# Generator function that combines jsons from folder and combines them..\n",
    "def generate_places(keyword):\n",
    " \n",
    "    file_path_df = get_file_paths_from_gcs_directory(keyword, bucket)\n",
    "    total_json_tables = len(file_path_df)\n",
    "    places_loaded = set()\n",
    "    \n",
    "    for idx, row in file_path_df.iterrows():\n",
    "        if idx % 50 == 0: # print progress after every 50 processed..\n",
    "            print(f'{idx} of {total_json_tables} json files processed')\n",
    "        tract_dict = get_json_per_file_path(row['file_path'], keyword, bucket)\n",
    "        places_list = tract_dict.get('results', [])\n",
    "        for place_dict in places_list:\n",
    "            if place_dict['place_id'] in places_loaded:\n",
    "                continue\n",
    "            places_loaded.add(place_dict['place_id'])\n",
    "            \n",
    "            cleaned_dict = { col : place_dict.get(col) for col in COLUMNS}\n",
    "            cleaned_dict.update({ col : place_dict.get('geometry', {}).get('location', {}).get(col) for col in ['lat', 'lng']})\n",
    "\n",
    "            yield cleaned_dict\n",
    "\n",
    "            \n",
    "def load_to_BQ(df, client, dataset, table, schema, cluster_fields=None):\n",
    "\n",
    "    table_id = f'{dataset}.{table}'\n",
    "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.autodetect = True\n",
    "\n",
    "    if cluster_fields is not None:\n",
    "        job_config.clustering_fields = cluster_fields\n",
    "\n",
    "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "    job.result()\n",
    "    out_table = client.get_table(table_id)\n",
    "    print(\"Loaded {} rows into {}.\".format(out_table.num_rows, table_id))\n",
    "\n",
    "def get_schema(df):\n",
    "    DTYPE_MAP = {'object':'STRING', 'float64': 'FLOAT64', 'int64':'INT64', 'bool':'BOOL', 'datetime64[ns]': 'TIMESTAMP'}\n",
    "    return [bigquery.SchemaField(c, DTYPE_MAP[str(d)]) for c,d in zip(df.columns, df.dtypes)]\n",
    "\n",
    "def correct_df_dtypes(df, inplace=False):\n",
    "    _df = df.copy(deep=(not inplace))\n",
    "    DTYPE_MAP = {'object':str, 'float64': float, 'int64':int, 'bool':bool}\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        _df[col] = df[col].astype(DTYPE_MAP[str(dtype)])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2d1c9-1609-4c0f-b4a6-65f35a13715e",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55b0a4-d4db-4dce-b0a2-4c02e67d8d4b",
   "metadata": {},
   "source": [
    "You can use the below functions to streamline everything to pull and store the data necessary. All you need to provide is the state, a county fips code (these functions conduct the search & pull per county), some keywords to search, and a scope. \n",
    "\n",
    "The scope might depend on what you are looking for. Generally, running scope as 'tract' will be more expensive than running as 'zip_code'. The Google Places API has a results limit of 60 places. If you believe there might be more than 60 of a certain amenity (your keyword) in a zipcode, you might consider running with scope as 'tract'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fda82-ab8d-4b52-8cfa-9a96d5cd75e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pulling multiple keywords at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de10b8-3e8f-49a4-9058-0fb0bd4ad403",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. Create a list of amenities you would like to pull for an area\n",
    "* ex. `google_amenities=['library','park','lake','art gallery','retail store','police station','parking']`\n",
    "\n",
    "2. Set GCS_DATA_DIR following below convention:\n",
    "* `GCS_DATA_DIR = 'nbhd_similarity/{STATE FIPS}{COUNTY FIPS}/'`\n",
    "\n",
    "3. Set state and county variables.\n",
    "* ex: `state = 'Georgia'\n",
    "        county = '063'`\n",
    "        \n",
    "4. Run gp_api_all_in_list() function with assigned parameter variables. \n",
    "* ex:`gp_api_all_in_list(google_amenities,state,county,scope='zip_code')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9a7dc2-f9c8-46ba-8e22-c1baa3389162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls all search results for given county for a certain keyword and stores the data in BQ\n",
    "def pull_store_data(state,county,keyword = 'restaurant',replace=False, scope='tract'):\n",
    "    \n",
    "    #check to see if data already exists in bigquery\n",
    "    if not replace and not is_bq_file_existing(state,county,keyword):\n",
    "        print('Building Table..')\n",
    "    #If does not already exist, build the table using previous functions\n",
    "        if scope =='zip_code':\n",
    "            locs = pull_county_zip_codes(state,county)\n",
    "        else:\n",
    "            locs = pull_census_tract_geodata(state,county)\n",
    "            \n",
    "        pull_keyword_results(locs, keyword=keyword, replace=replace,scope=scope)\n",
    "        results = pd.DataFrame(generate_places(keyword=keyword),columns = COLUMNS)\n",
    "        # results.head()\n",
    "        load_to_BQ(correct_df_dtypes(results),\n",
    "                   client,\n",
    "                   'nbhd_similarity', \n",
    "                   f'gmaps_{keyword.lower().replace(\" \", \"_\")}_{str(states.lookup(state).fips) + str(county)}', \n",
    "                   get_schema(correct_df_dtypes(results))\n",
    "                  )\n",
    "        return (f'data stored in nbhd_similarity.gmaps_{keyword.lower().replace(\" \", \"_\")}_{str(states.lookup(state).fips) + str(county)}')\n",
    "\n",
    "#checks if file already exists in BigQuery    \n",
    "def is_bq_file_existing(state,county,keyword = 'restaurant'):\n",
    "    table_id = f'homevest-data.nbhd_similarity.gmaps_{keyword.lower().replace(\" \", \"_\")}_{str(states.lookup(state).fips) + str(county)}'\n",
    "    \n",
    "    try:\n",
    "        client.get_table(table_id)  # Make an API request.\n",
    "        print(f'Table already built as {table_id}')\n",
    "        return True\n",
    "    except NotFound:\n",
    "        print('Table needs to be built..')\n",
    "        return False\n",
    "\n",
    "def gp_api_all_in_list(amenities,state,county,scope='tract'):\n",
    "    for a in amenities:\n",
    "        pull_store_data(state,county,keyword = a, scope=scope)\n",
    "    print('All Tables Built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce16cc83-925c-427d-8ea6-81e5a9629d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL EXAMPLE OF ABOVE. DO NOT RUN THIS BECAUSE WE ALREADY HAVE THIS DATA.\n",
    "# marion county: 213 places \n",
    "# GCS_DATA_DIR = 'nbhd_similarity/13097/' #change county here everytime\n",
    "\n",
    "# state = 'Indiana'\n",
    "# county = '097'\n",
    "# gp_api_all_in_list(google_amenities,state,county,scope='zip_code')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
